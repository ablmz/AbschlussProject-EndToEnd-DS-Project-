{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model with The Data of Clinics in Lower Saxony Germany\n",
    "\n",
    "\n",
    "\n",
    "![Machine Learning Workflow](https://miro.medium.com/max/2400/1*QV1rVgh3bfaMbtxueS-cgA.png)<br>\n",
    "Foto:https://miro.medium.com/max/2400/1*QV1rVgh3bfaMbtxueS-cgA.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erforderliche Importe,die benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "#import csv\n",
    "\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "from textblob_de import TextBlobDE\n",
    "from textblob_de import PatternParser\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "import bar_chart_race as bcr\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#from bokeh.io import output_notebook\n",
    "#output_notebook()\n",
    "\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 300\n",
    "pd.options.display.max_colwidth = 100\n",
    "np.set_printoptions(threshold=2000)\n",
    "#import holoviews as hv\n",
    "#import hvplot\n",
    "#from hvplot import pandas\n",
    "#hv.extension(\"bokeh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Konvertieren csv-Daten in Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\name\\Desktop\\WB\\Abschlussproject\\klinikbewertung.de\\alldata\\alldata_klinikandgoogle.csv', encoding='utf-8') #https://github.com/ablmz/Final-Project/tree/adem/klinikbewertung.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wir erstellen Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns= ['Name der Klinik', 'Textuelle Bewertung','Sternebewertung','Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for q in range(result.shape[0]):\n",
    "# create a list of our conditions\n",
    "conditions = [\n",
    "    (df['Sternebewertung'] >= 3 ),\n",
    "    (df['Sternebewertung'] < 3 )\n",
    "    ]\n",
    "# create a list of the values we want to assign for each condition\n",
    "values = [1, 0]\n",
    "\n",
    "# create a new column and use np.select to assign values to it using our lists as arguments\n",
    "df['Rating'] = np.select(conditions, values)\n",
    "\n",
    "# display updated DataFrame\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obwohl wir die Daten als UTF-8 kodiert haben, sollen wir einige Falsche Wörter ändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\name\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = SnowballStemmer(\"german\")\n",
    "# stop_words = set(stopwords.words(\"german\"))\n",
    "\n",
    "def clean_text(text, for_embedding=False):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "        - some encoding errors \n",
    "    \"\"\"\n",
    "#     RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "#     RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "#     RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "#     RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "#     if for_embedding:\n",
    "#         # Keep punctuation\n",
    "#         RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "#         RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "#     text = re.sub(RE_TAGS, \" \", text)\n",
    "#     text = re.sub(RE_ASCII, \" \", text)\n",
    "#     text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "#     text = re.sub(RE_WSPACE, \" \", text)\n",
    "    text= text.str.replace('\\n','')\n",
    "    text= text.str.replace('Ã¶','ö')\n",
    "    text= text.str.replace('Ã\\x84','Ä')\n",
    "    text= text.str.replace('Ã¤','ä')\n",
    "    text= text.str.replace('Ã¼','ü')\n",
    "    text= text.str.replace('Ã\\x9f','ß')\n",
    "    text= text.str.replace('Ã\\x9c','Ü')\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Compute unique word vector with frequencies\n",
    "exclude very uncommon (<10 obsv.) and common (>=30%) words\n",
    "use pairs of two words (ngram)\n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"word\", max_df=0.3, min_df=10, ngram_range=(1, 2), norm=\"l2\"\n",
    ")\n",
    "vectorizer.fit(df[\"Textuelle Bewertung\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter that needs explanation is the ngram_range. An ngram of one means that you look at each word separately. An ngram of two (or bigram) means that you take the preceding and following word into account as well. Thus, some context is added. This is helpful because then a model can learn that \"good\" and \"not good\" are different. In our case, in addition to using each word by itself we also add bigrams to make use of context. Let's see some of the created ngrams and their indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique word (ngram) vector extract:\n",
      "\n",
      " müssen und        5606\n",
      "stunden später    7258\n",
      "über die          9091\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Vector representation of vocabulary\n",
    "word_vector = pd.Series(vectorizer.vocabulary_).sample(3, random_state=1)\n",
    "print(f\"Unique word (ngram) vector extract:\\n\\n {word_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a numeric representation of the ngrams in our corpus. We see that the vectorizer also uses bigrams in addition to single words. The word \"müssen und\" is represented by the number 5606, while the number 7258 stands for the bigram \"stunden später\" and so on.\n",
    "This is only the first part of our text to numeric process. Before we can move on to transform each sentence to a vector of TF-IDF values, we need to prepare the data for the modeling part first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "To test the classification performance of our model, we will perform a cross validation. For that, we split our data into a training and a testing set. The former is used to train the model and the latter to evaluate its predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3927,)\n",
      "(1309,)\n"
     ]
    }
   ],
   "source": [
    "# Sample data - 25% of data to test set\n",
    "train, test = train_test_split(df, random_state=1, test_size=0.25, shuffle=True)\n",
    "\n",
    "X_train = train[\"Textuelle Bewertung\"]\n",
    "Y_train = train[\"Rating\"]\n",
    "X_test = test[\"Textuelle Bewertung\"]\n",
    "Y_test = test[\"Rating\"]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set consists of more than 3927 rows and the testing will be performed on more than 1309 observations. Now, that we have split our data we can transform the text data into its `TF-IDF` representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3927, 9132)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform each sentence to numeric vector with tf-idf value as elements\n",
    "X_train_vec = vectorizer.transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "X_train_vec.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each of our sentences is now represented by a vector of length 9132. It might be interesting to compare text and numeric representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "['Absolut zufrieden mit dem Team , von dem ich am 30.09.2011 um 13.00 ambulant operiert wurde. Die Krankenschwester waren auch sehr gut. Danke!']\n",
      "\n",
      "Vector representation of sentence:\n",
      "          00        09        13      2011        30  absolut        am  \\\n",
      "0  0.185898  0.214235  0.196529  0.201964  0.161042  0.15359  0.092103   \n",
      "\n",
      "   ambulant  auch sehr    danke       dem   dem ich  dem team  \\\n",
      "0  0.198253   0.171252  0.13431  0.173229  0.204667  0.237733   \n",
      "\n",
      "   die krankenschwester    ich am  krankenschwester   mit dem  operiert  \\\n",
      "0              0.227242  0.197669          0.185069  0.131094  0.129453   \n",
      "\n",
      "   operiert wurde  sehr gut      team  team von        um     um 13   von dem  \\\n",
      "0        0.245439  0.109134  0.127542  0.227242  0.098073  0.258141  0.194352   \n",
      "\n",
      "      waren  waren auch  wurde die  zufrieden  zufrieden mit  \n",
      "0  0.092489     0.22125   0.192297   0.130723       0.206097  \n"
     ]
    }
   ],
   "source": [
    "# Compare original comment text with its numeric vector representation\n",
    "print(f\"Original sentence:\\n{X_train[3:4].values}\\n\")\n",
    "# Feature Matrix\n",
    "features = pd.DataFrame(\n",
    "    X_train_vec[3:4].toarray(), columns=vectorizer.get_feature_names()\n",
    ")\n",
    "nonempty_feat = features.loc[:, (features != 0).any(axis=0)]\n",
    "print(f\"Vector representation of sentence:\\n {nonempty_feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this five word sentence, the vector of length  9132 contains mostly zeros. However, the indices representing the used words / ngrams are non empty. They include the value that TF-IDF assigned to them. In this particular case, \"um 13\" (at 13 o'Clock) has the largest weight meaning that it is relatively frequent in our sentence while not being very common in other sentences of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have prepared our features, we can start to train and evaluate models. For a binary classification task there are many options to chose from in scikit-learn. We will focus on the ones that are most promising. In my experience they are: Logistic Regression, Support Vector Classification (SVC), Ensemble Methods (Boosting, Random Forest) and Neural Networks (i.e. Multi Layer Perceptron or MLP in sklearn). We will compare these models and chose the most promising one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers to test: ['LogisticRegression', 'LinearSVC', 'RandomForestClassifier', 'XGBClassifier', 'MLPClassifier']\n"
     ]
    }
   ],
   "source": [
    "# models to test\n",
    "classifiers = [\n",
    "    LogisticRegression(solver=\"sag\", random_state=1),\n",
    "    LinearSVC(random_state=1),\n",
    "    RandomForestClassifier(random_state=1),\n",
    "    XGBClassifier(random_state=1, use_label_encoder =False),\n",
    "    MLPClassifier(\n",
    "        random_state=1,\n",
    "        solver=\"adam\",\n",
    "        hidden_layer_sizes=(12, 12, 12),\n",
    "        activation=\"relu\",\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=1,\n",
    "    ),\n",
    "]\n",
    "# get names of the objects in list (too lazy for c&p...)\n",
    "names = [re.match(r\"[^\\(]+\", name.__str__())[0] for name in classifiers]\n",
    "print(f\"Classifiers to test: {names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier: LogisticRegression\n",
      "Training classifier: LinearSVC\n",
      "Training classifier: RandomForestClassifier\n",
      "Training classifier: XGBClassifier\n",
      "[22:24:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training classifier: MLPClassifier\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test all classifiers and save pred. results on test data\n",
    "results = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    print(f\"Training classifier: {name}\")\n",
    "    clf.fit(X_train_vec, Y_train)\n",
    "    prediction = clf.predict(X_test_vec)\n",
    "    report = sklearn.metrics.classification_report(Y_test, prediction)\n",
    "    results[name] = report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LogisticRegression and LinearSVC was very fast while the remaining classifiers were significantly slower. This has to do with their higher model complexity but can also greatly vary depending on the parameters used.\n",
    "After having trained all our models on the train data and applying their prediction on the test data, we can judge their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LogisticRegression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91       515\n",
      "           1       0.93      0.96      0.95       794\n",
      "\n",
      "    accuracy                           0.93      1309\n",
      "   macro avg       0.93      0.93      0.93      1309\n",
      "weighted avg       0.93      0.93      0.93      1309\n",
      "\n",
      "\n",
      "Results for LinearSVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       515\n",
      "           1       0.95      0.94      0.94       794\n",
      "\n",
      "    accuracy                           0.93      1309\n",
      "   macro avg       0.93      0.93      0.93      1309\n",
      "weighted avg       0.93      0.93      0.93      1309\n",
      "\n",
      "\n",
      "Results for RandomForestClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       515\n",
      "           1       0.92      0.93      0.93       794\n",
      "\n",
      "    accuracy                           0.91      1309\n",
      "   macro avg       0.91      0.91      0.91      1309\n",
      "weighted avg       0.91      0.91      0.91      1309\n",
      "\n",
      "\n",
      "Results for XGBClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87       515\n",
      "           1       0.92      0.91      0.91       794\n",
      "\n",
      "    accuracy                           0.89      1309\n",
      "   macro avg       0.89      0.89      0.89      1309\n",
      "weighted avg       0.89      0.89      0.89      1309\n",
      "\n",
      "\n",
      "Results for MLPClassifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90       515\n",
      "           1       0.91      0.97      0.94       794\n",
      "\n",
      "    accuracy                           0.93      1309\n",
      "   macro avg       0.93      0.91      0.92      1309\n",
      "weighted avg       0.93      0.93      0.93      1309\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction results\n",
    "for k, v in results.items():\n",
    "    print(f\"Results for {k}:\")\n",
    "    print(f\"{v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of sci-kit's classification_report provides us with several metrics. For unbalanced data sets accuracy is an inappropriate metric. Since it's value solely tells you how many cases have been properly classified, a high value can be achieved by always predicting the majority class. In contrast, the f1-score puts precision and recall of each class in relation to each other. As such, it is a more fine grained measure. We can use an aggregate version of it to have a single metric summarizing the performance of each model. For example, the weighted f1-score is an average of the f1-scores of both our classes taking the class distribution into account. On the other hand, the macro f1-score averages over class scores without weighing them. Consequently, we'll use that as we wish to give the same importance to both our classes. This is because even though bad grades are much more rare they also have a more severe impact.\n",
    "All methods achieve impressive results predicting the good ratings class with f1-scores above 0.89. However, results for the bad ratings class are much lower and vary wildly.\n",
    "Here, the ensemble methods, i.e. RandomForest and XGBoost, perform worst. However, with some more effort to tune their parameters they would probably fare significantly better.\n",
    "As a general rule of thumb, logistic regressions deliver decent results in many different use cases. Moreover, they are simple to apply, robust and computationally efficient. macro f1-score we have three best score, MLP, LR and SVC over 0.92. In general, SVC is comparable in speed and simplicity to the logistic regression. In addition, it often performs very well in classification tasks as we can see here. In contrast, Neural Networks perform extremely well in all sorts of tasks but are also much more complex and slow all around. Because of this, we will stick to the LinearSVC classifier for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning\n",
    "\n",
    "We've learned that linear SVC is a solid model choice and delivers great results out of the box. Still, we might be able to improve upon those by taking a more guided approach to choosing parameters. To do so, we will compare different parameters for feature creation as well as modeling. We can achieve this by making use of the pipeline and grid search functionality in sci-kit learn.  \n",
    "The `Pipeline` object encapsulates several processing steps into one. The last step in a pipeline must have a `fit()` functionality. The previous steps a `transform()` and `fit()` functionality. The `Pipeline` object itself has the same `fit()`, `transform()` and `predict()` methods as any other model in scikit-learn. Thus, we can streamline the whole process of feature creation, model fitting and prediction into one step.  \n",
    "With `GridSearchCV` we can define parameter spaces for our functions. All parameter combinations will be evaluated against one another in a cross validation using a defined score metric. Next, we combine our pipeline with grid search. In result, we can jointly test combinations of parameters in feature creation and model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"svc\", LinearSVC())])\n",
    "\n",
    "# define parameter space to test # runtime 35min\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2), (1, 3)],\n",
    "    \"tfidf__max_df\": np.arange(0.3, 0.8, 0.2),\n",
    "    \"tfidf__min_df\": np.arange(5, 100, 45),\n",
    "}\n",
    "pipe_clf = GridSearchCV(pipe, params, n_jobs=-1, scoring=\"f1_macro\")\n",
    "pipe_clf.fit(X_train, Y_train)\n",
    "pickle.dump(pipe_clf, open(\"./clf_pipe.pck\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value added to the `params` to be tested with `GridSearchCV` increases the run time of the training significantly. To limit the combinations, we first optimize the feature creation step. We test for different values for `ngram`. Moreover, `max_df` and `min_df` set an upper and lower limit for word frequencies. We want to exclude infrequent words because the model won't be able to learn (meaningful) associations with very few observations. The same is true for very frequent words which won't allow the model to differentiate between classes. For each parameter we check the values which returned the best model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidf__max_df': 0.3, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(pipe_clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using this best parameters for TF-IDF we can search for optimal parameters for the LinearSVC classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# feature creation and modelling in a single function\n",
    "pipe = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"svc\", LinearSVC())])\n",
    "\n",
    "# define parameter space to test # runtime 19min\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 3)],\n",
    "    \"tfidf__max_df\": [0.5],\n",
    "    \"tfidf__min_df\": [5],\n",
    "    \"svc__C\": np.arange(0.2, 1, 0.15),\n",
    "}\n",
    "pipe_svc_clf = GridSearchCV(pipe, params, n_jobs=-1, scoring=\"f1_macro\")\n",
    "pipe_svc_clf.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "#pickle.dump(pipe_svc_clf, open(\"./pipe_svc_clf.pck\", \"wb\")) first version\n",
    "\n",
    "pickle.dump(pipe_svc_clf, open(\"./pipe_svc_clf.pkl\", \"wb\"))\n",
    "\n",
    "model = pickle.load(open(\"./pipe_svc_clf.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = pipe_svc_clf.best_params_\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on the C parameter which is basically a regularization parameter and essential in the performance of the SVC classifier. High values of C mean the margin of the hyperplane chosen by SVC to separate the data will be smaller. Thus, while classification on training data will be better this can also lead to overfitting. Consequently, C controls a trade off between a low training and testing error.\n",
    "\n",
    "Finally, we combine these best parameters and test the prediction of our model using the pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93       515\n",
      "           1       0.95      0.95      0.95       794\n",
      "\n",
      "    accuracy                           0.94      1309\n",
      "   macro avg       0.94      0.94      0.94      1309\n",
      "weighted avg       0.94      0.94      0.94      1309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run pipe with optimized parameters\n",
    "pipe.set_params(**best_params).fit(X_train, Y_train)\n",
    "pipe_pred = pipe.predict(X_test)\n",
    "report = sklearn.metrics.classification_report(Y_test, pipe_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using parameter tuning we have slightly improved on our already decent model by increasing the precision for class 1.0. However, we can see that the margin for improvement is little. On one hand, this is because our initial parameters were already very close to the optimum. On the other hand, it might be that given our data and the approach used we might already be close to a barrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipe, open(\"./pipe.pkl\", \"wb\"))\n",
    "model = pickle.load(open(\"./pipe.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Now, that we have our best performing model (so far) we can use it to make predictions. One possible application is to find contradictory reviews, i.e. reviews where the sentiment of the comment doesn't match the rating. For that, we look at cases where our model makes a prediction with high confidence which doesn't match the original rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get confidence score for prediction\n",
    "# conf_score = pipe.decision_function(X_test)\n",
    "# # Get the Nth highest / lowest score\n",
    "# # high score indicates class 1 (bad), low score 0 (good)\n",
    "# score_neg = np.sort(conf_score)[-1300]\n",
    "# score_pos = np.sort(conf_score)[1300]\n",
    "# # print(\n",
    "# #     f\"Threshold for negative rating: {score_neg}\\nThreshold for positive rating: {score_pos}\"\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `decision_function` returns values ranging from negative to positive (without a general boundary). They depict the distance of the input to the hyperplane which the SVM algorithm uses to separate the two classes. In our case, large values indicate a higher confidence in class 1 (i.e. a bad rating) and low values in class 0. Consequently, we take the n highest (lowest) values as a threshold for a high confidence in class 1 (0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.options.display.max_colwidth = 800\n",
    "# Predicted good but rated bad\n",
    "#test[[\"Rating\", \"Textuelle Bewertung\"]][(Y_test != pipe_pred) & (conf_score <= score_pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted bad but rated good\n",
    "#test[[\"Rating\", \"Textuelle Bewertung\"]][(Y_test != pipe_pred) & ((conf_score >= score_neg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new comments from website that were not included in original data\n",
    "INPUT = [ \"Nach meiner Beobachtung hat diese Praxis eine schlechte Hygiene.\"]\n",
    "#          \"Sprechstundenhilfe war super nett man fühlt sich wohl.\"\n",
    "#          \"Super sympathische Ärztin, fühle mich bei ihr bestens aufgehoben.\"\n",
    "#         \"nicht zuverlässig und nicht vertrauenswürdig.\"]\n",
    "#          \"Frau Doktor Merz nimmt sich richtig Zeit für mich. Hilft wo sie kann.\"\n",
    "#          \"Hört wirklich einen zu. Sehr nett und freundlich. Sie ist sehr kompetent,\"\n",
    "#          \"zuverlässig und vertrauenswürdig.\",\n",
    "#          \"Nach meiner Beobachtung hat diese Praxis eine schlechte Hygiene. \",\n",
    "#          \"Mangels akriebischer Behandlung musste mehrmals nachgebessert werden.\"\n",
    "#          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Process comments as we did with train data\n",
    "#text = [clean_text(comment) for comment in INPUT]\n",
    "#text = [comment for comment in INPUT]\n",
    "text=INPUT\n",
    "#text_out = \" \\n\".join(text)\n",
    "#print(f\"Input after pre-processing / cleaning:\\n\\n{text_out}\")\n",
    "# run comments through pipe: predict using our best model from above\n",
    "predictions = pipe.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Schlechte Erfahrung'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show comment and predicted Labels\n",
    "predictions = pd.Series(predictions)\n",
    "predictions = predictions.replace(0, \"Schlechte Erfahrung\").replace(1, \"Gute Erfahrung\")\n",
    "\n",
    "# pd.concat(\n",
    "#     [pd.Series(INPUT), predictions], axis=\"columns\", keys=[\"Kommentar\", \"Hervorsage\"]\n",
    "# )\n",
    "\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
